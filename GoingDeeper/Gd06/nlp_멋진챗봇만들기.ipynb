{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4469df8",
   "metadata": {},
   "source": [
    "# [프로젝트] 멋진챗봇만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2984b",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드\n",
    "### Step 2. 데이터 정제\n",
    "### Step 3. 데이터 토큰화\n",
    "### Step 4. Augmentation\n",
    "### Step 5. 데이터 벡터화\n",
    "### Step 6. 훈련하기\n",
    "### Step 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da239fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4f1a6",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드\n",
    "ChatbotData.csv 사용, 질문(questions), 답변(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a30ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
      "총 질문 수: 11823\n",
      "총 답변 수: 11823\n",
      "첫 번째 질문: 12시 땡!\n",
      "첫 번째 답변: 하루가 또 가네요.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chatbot.csv 파일 경로 설정 (로컬 경로를 지정해 주세요)\n",
    "file_path = \"Chatbot.csv\"\n",
    "\n",
    "# CSV 파일 읽기\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터 확인 (첫 5줄)\n",
    "print(data.head())\n",
    "\n",
    "# 질문과 답변을 각각 분리해서 저장\n",
    "questions = data['Q'].tolist()  # 질문 열(Q)\n",
    "answers = data['A'].tolist()    # 답변 열(A)\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"총 질문 수: {len(questions)}\")\n",
    "print(f\"총 답변 수: {len(answers)}\")\n",
    "\n",
    "# 첫 번째 질문과 답변 출력\n",
    "print(f\"첫 번째 질문: {questions[0]}\")\n",
    "print(f\"첫 번째 답변: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3832e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions 샘플: ['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
      "answers 샘플: ['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.']\n"
     ]
    }
   ],
   "source": [
    "questions = data['Q'].tolist()  # 질문 열 이름 확인 후 사용\n",
    "answers = data['A'].tolist()    # 답변 열 이름 확인 후 사용\n",
    "\n",
    "print(\"questions 샘플:\", questions[:5])\n",
    "print(\"answers 샘플:\", answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d1996",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "- 영문자의 경우, 모두 소문자로 변환합니다.\n",
    "- 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571c0cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후: hello! 내 이름은 김민혁입니다, 특수문자 !\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 1. 영문자를 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 2. 영문자, 한글, 숫자, 주요 특수문자를 제외한 모든 문자를 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9가-힣?.!,]+\", \" \", sentence)\n",
    "    \n",
    "    # 3. 문자열 양 끝의 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 예시 데이터로 함수 테스트\n",
    "test_sentence = \"Hello! 내 이름은 김민혁입니다, 특수문자 !@#$%\"\n",
    "preprocessed_sentence = preprocess_sentence(test_sentence)\n",
    "print(\"전처리 후:\", preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af6951",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화\n",
    "> 토큰화(mecab), build_corpus() 함수 구현\n",
    "- 소스 문장 데이터와 타겟 문장 데이터를 입력\n",
    "- 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화\n",
    "- 토큰화는 전달받은 토크나이즈 함수를 사용, mecab.morphs 함수를 전달\n",
    "- 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외\n",
    "- 중복되는 문장은 데이터에서 제외\n",
    "    - 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0564abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def build_corpus(sentences, tokenizer, max_len=40):\n",
    "    corpus = []\n",
    "\n",
    "    seen_sentences = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Step 1: 문장 정제\n",
    "        sentence = preprocess_sentence(sentence)\n",
    "\n",
    "        # Step 2: 토큰화\n",
    "        tokens = tokenizer(sentence)\n",
    "\n",
    "        # Step 3: 토큰의 개수가 max_len보다 길면 제외\n",
    "        if len(tokens) > max_len:\n",
    "            continue\n",
    "\n",
    "        # Step 4: 중복 문장 제거\n",
    "        tokenized_sentence = ' '.join(tokens)\n",
    "        if tokenized_sentence in seen_sentences:\n",
    "            continue\n",
    "        seen_sentences.add(tokenized_sentence)\n",
    "\n",
    "        # 최종 토큰화된 문장을 corpus에 추가\n",
    "        corpus.append(tokens)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "# que_corpus 와 ans_corpus 생성\n",
    "que_corpus = build_corpus(questions, mecab.morphs, max_len=40)\n",
    "ans_corpus = build_corpus(answers, mecab.morphs, max_len=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c77ba",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation\n",
    "한국어로 사전 훈련된 Embedding model 다운로드 - ko.bin 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b089ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# gensim 다운그레이드\n",
    "!pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c6ca4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료!\n",
      "[('학교의', 0.7560996413230896), ('강습소', 0.7425637245178223), ('중고등학교', 0.7386142015457153), ('전문학교', 0.7356827855110168), ('사립학교', 0.7347193956375122), ('소학교', 0.7305554747581482), ('여학교', 0.7091007232666016), ('사범학교', 0.6901223659515381), ('대학', 0.6897724866867065), ('학원', 0.6869212985038757)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_249/691456926.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  print(wv.most_similar(\"학교\"))  # \"학교\"와 유사한 단어 출력\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# ko.bin 파일 경로 설정\n",
    "model_path = 'ko.bin'\n",
    "\n",
    "# Word2Vec 모델 로드\n",
    "wv = Word2Vec.load(model_path)\n",
    "print(\"모델 로드 완료!\")\n",
    "\n",
    "# 모델 테스트 예시\n",
    "print(wv.most_similar(\"학교\"))  # \"학교\"와 유사한 단어 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5cebc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_249/2324208733.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  wv.most_similar(\"바나나\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('코코넛', 0.8097119927406311),\n",
       " ('시금치', 0.7701147794723511),\n",
       " ('레몬', 0.76884925365448),\n",
       " ('땅콩', 0.7684735059738159),\n",
       " ('파인애플', 0.7639915347099304),\n",
       " ('녹차', 0.7631460428237915),\n",
       " ('딸기', 0.7617197036743164),\n",
       " ('바닐라', 0.7497864961624146),\n",
       " ('파슬리', 0.7447543144226074),\n",
       " ('코코아', 0.7408244609832764)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"바나나\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e418a101",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11823 [00:00<?, ?it/s]/tmp/ipykernel_249/3065843551.py:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in model and random.random() < prob:\n",
      "/tmp/ipykernel_249/3065843551.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  similar_words = model.most_similar(word, topn=5)\n",
      "100%|██████████| 11823/11823 [01:06<00:00, 178.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 질문 데이터 개수: 11823\n",
      "증강된 질문 데이터 개수: 35469\n",
      "전체 질문 데이터 개수: 47292\n",
      "원본 답변 데이터 개수: 11823\n",
      "증강된 답변 데이터 개수: 35469\n",
      "전체 답변 데이터 개수: 47292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# lexical_sub 함수 정의: 단어를 유사도가 높은 단어로 치환하여 데이터 증강\n",
    "def lexical_sub(sentence, model, prob=0.7):\n",
    "    words = sentence.split()\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model and random.random() < prob:\n",
    "            try:\n",
    "                similar_words = model.most_similar(word, topn=5)\n",
    "                new_word = random.choice(similar_words)[0]\n",
    "                new_words.append(new_word)\n",
    "            except KeyError:\n",
    "                new_words.append(word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# 데이터 증강 수행 함수\n",
    "def augment_corpus(questions, answers, model):\n",
    "    augmented_questions = []\n",
    "    augmented_answers = []\n",
    "\n",
    "    for que, ans in tqdm(zip(questions, answers), total=len(questions)):\n",
    "        # 1. 질문을 증강하고 답변은 원본 유지\n",
    "        augmented_questions.append(lexical_sub(que, model))  # 개별 질문\n",
    "        augmented_answers.append(ans)  # 개별 답변\n",
    "\n",
    "        # 2. 답변을 증강하고 질문은 원본 유지\n",
    "        augmented_questions.append(que)\n",
    "        augmented_answers.append(lexical_sub(ans, model))\n",
    "        \n",
    "        # 3. 질문과 답변을 모두 증강\n",
    "        augmented_questions.append(lexical_sub(que, model))\n",
    "        augmented_answers.append(lexical_sub(ans, model))\n",
    "\n",
    "    # 최종 데이터셋 생성\n",
    "    final_questions = questions + augmented_questions\n",
    "    final_answers = answers + augmented_answers\n",
    "\n",
    "    print(f\"원본 질문 데이터 개수: {len(questions)}\")\n",
    "    print(f\"증강된 질문 데이터 개수: {len(augmented_questions)}\")\n",
    "    print(f\"전체 질문 데이터 개수: {len(final_questions)}\")\n",
    "\n",
    "    print(f\"원본 답변 데이터 개수: {len(answers)}\")\n",
    "    print(f\"증강된 답변 데이터 개수: {len(augmented_answers)}\")\n",
    "    print(f\"전체 답변 데이터 개수: {len(final_answers)}\")\n",
    "\n",
    "    return final_questions, final_answers\n",
    "\n",
    "# 실제 데이터에 증강 적용\n",
    "final_questions, final_answers = augment_corpus(questions, answers, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca3ff8",
   "metadata": {},
   "source": [
    "### 오류 🚨🚨🚨\n",
    "앞에서 questions와 answers를 잘 불러왔다가 중간에 내가 \n",
    "\n",
    "`questions=[]`\n",
    "`answers =[]`\n",
    "\n",
    "를 선언해서 정제했던 데이터를 다 날렸길래 데이터 수가 다 0으로 지정되어졌다.\n",
    "\n",
    "그래서 데이터 증강이 실현되지 않았던 걸 확인했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509d1cf",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화\n",
    "- `<start>` 토큰과 `<end>` 토큰\n",
    "- `que_corpus` 와 결합하여 전체 데이터에 대한 단어 사전을 구축\n",
    "- 벡터화하여 `enc_train` 과 `dec_train` 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7d85239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '12', '시', '땡', '!', '<end>']\n",
      "단어 사전 크기: 25118\n",
      "인코더 데이터 크기: (47292, 50)\n",
      "디코더 데이터 크기: (47292, 50)\n"
     ]
    }
   ],
   "source": [
    "# 답변 데이터에 <start>와 <end> 토큰 추가\n",
    "ans_corpus_with_tokens = [[\"<start>\"] + ans.split() + [\"<end>\"] for ans in final_answers]\n",
    "\n",
    "# 결과 예시 확인\n",
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])\n",
    "\n",
    "# 2. que_corpus와 ans_corpus를 결합하여 단어 사전 생성\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 모든 데이터를 단어 사전에 포함시키기 위해 que_corpus와 ans_corpus_with_tokens 결합\n",
    "total_corpus = final_questions + ans_corpus_with_tokens\n",
    "\n",
    "# 토크나이저 초기화 및 단어 사전 생성\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(total_corpus)\n",
    "\n",
    "# 단어 사전 크기 확인\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 패딩 토큰 포함\n",
    "\n",
    "print(\"단어 사전 크기:\", vocab_size)\n",
    "\n",
    "# 3. 각 문장을 시퀀스 번호로 변환하여 벡터화\n",
    "# 질문과 답변 각각에 대해 벡터화 수행\n",
    "enc_train = tokenizer.texts_to_sequences(final_questions)\n",
    "dec_train = tokenizer.texts_to_sequences(ans_corpus_with_tokens)\n",
    "\n",
    "# 4. 패딩 처리\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 최대 길이 설정\n",
    "max_len = 50  # 필요에 따라 수정 가능\n",
    "\n",
    "enc_train = pad_sequences(enc_train, maxlen=max_len, padding='post')\n",
    "dec_train = pad_sequences(dec_train, maxlen=max_len, padding='post')\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"인코더 데이터 크기: {enc_train.shape}\")\n",
    "print(f\"디코더 데이터 크기: {dec_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6b83d",
   "metadata": {},
   "source": [
    "## Step 6. 훈련하기\n",
    "- Transformer 모델 설계\n",
    "    - Positional Encoding\n",
    "    - Mask generate\n",
    "    - Multi-Head Attention\n",
    "    - Position-wise Feed-Forward Network\n",
    "    - Encoder Layer\n",
    "    - Decoder Layer\n",
    "    - Encoder\n",
    "    - Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b8263",
   "metadata": {},
   "source": [
    "Positonal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74cd9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d298d0",
   "metadata": {},
   "source": [
    "mask 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5322fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6ba6e",
   "metadata": {},
   "source": [
    "Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ab30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567dec5",
   "metadata": {},
   "source": [
    "Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b406b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5ca64",
   "metadata": {},
   "source": [
    "Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b8d708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57434943",
   "metadata": {},
   "source": [
    "Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0e4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f02e7",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d5e8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206228f8",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496d19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6306fc",
   "metadata": {},
   "source": [
    "Transformer 조립하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "299f35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab49af",
   "metadata": {},
   "source": [
    "모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d781b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f33c88",
   "metadata": {},
   "source": [
    "Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e14486a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19ead4",
   "metadata": {},
   "source": [
    "Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c80fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81ae7b",
   "metadata": {},
   "source": [
    "Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efdeaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8becf51",
   "metadata": {},
   "source": [
    "Time Step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72071dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ffd03",
   "metadata": {},
   "source": [
    "번역 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fc2c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer, max_len=50):\n",
    "    sentence = preprocess_sentence(sentence)  # 전처리 함수 사용\n",
    "    tokens = src_tokenizer.texts_to_sequences([sentence])[0]  # 토큰화\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=max_len, padding='post')\n",
    "\n",
    "    output = tf.expand_dims([tgt_tokenizer.word_index['<start>']], 0)\n",
    "    ids = []\n",
    "\n",
    "    for i in range(max_len):\n",
    "        predictions, _, _, _ = model(_input, output, None, None, None)\n",
    "        predicted_id = tf.argmax(predictions[0, -1]).numpy()\n",
    "\n",
    "        if predicted_id == tgt_tokenizer.word_index['<end>']:\n",
    "            break\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    return ' '.join([tgt_tokenizer.index_word[id] for id in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f4297",
   "metadata": {},
   "source": [
    "🏃🏃🏃💨 훈련 시작!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c6a5ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 738/738 [02:15<00:00,  5.44batch/s, Loss=4.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.9479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 738/738 [02:19<00:00,  5.28batch/s, Loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.1867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 738/738 [02:23<00:00,  5.14batch/s, Loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 738/738 [02:23<00:00,  5.13batch/s, Loss=0.851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 738/738 [02:23<00:00,  5.14batch/s, Loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.7018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 738/738 [02:23<00:00,  5.14batch/s, Loss=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.6056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 738/738 [02:23<00:00,  5.14batch/s, Loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.5449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 738/738 [02:23<00:00,  5.15batch/s, Loss=0.493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.4926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 738/738 [02:23<00:00,  5.15batch/s, Loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.4604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 738/738 [02:23<00:00,  5.15batch/s, Loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.4287\n",
      "\n",
      "질문: 지루하다, 놀러가고 싶어.\n",
      "예측 답변: 좋은 사람 만날 수 있을 거예요.\n",
      "\n",
      "질문: 오늘 일찍 일어났더니 피곤하다.\n",
      "예측 답변: 잘못 주무셨나봐요.\n",
      "\n",
      "질문: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "예측 답변: 말하기 힘들었겠어요.\n",
      "\n",
      "질문: 집에 있는다는 소리야.\n",
      "예측 답변: 좋은 곳으로 데려다 깁 거예요.\n",
      "🫡 훈련 및 번역 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(final_questions)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 단어 사전 크기\n",
    "\n",
    "# 1. Dataset 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# 모델 및 옵티마이저 초기화\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True\n",
    ")\n",
    "learning_rate = LearningRateScheduler(512, warmup_steps=1000)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# 2. 훈련 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    dataset_count = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count, desc=f'Epoch {epoch + 1}', unit='batch')\n",
    "\n",
    "    for (batch, (src, tgt)) in enumerate(dataset):\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += loss\n",
    "        tqdm_bar.set_postfix({'Loss': total_loss.numpy() / (batch + 1)})\n",
    "        tqdm_bar.update(1)\n",
    "    \n",
    "    tqdm_bar.close()\n",
    "    print(f'Epoch {epoch + 1} Loss: {total_loss.numpy() / dataset_count:.4f}')\n",
    "\n",
    "# 예문 번역 생성\n",
    "test_questions = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n질문: {question}\")\n",
    "    predicted_answer = evaluate(question, transformer, tokenizer, tokenizer)\n",
    "    print(f\"예측 답변: {predicted_answer}\")\n",
    "\n",
    "print(\"🫡 훈련 및 번역 생성 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa1a78",
   "metadata": {},
   "source": [
    "## Step 7. 성능 측정하기\n",
    "올바른 대답을 하는지 중요한 것이 평가 지표이다.\n",
    "\n",
    "주어진 질문에 적절한 답변을 하는지 확인하는 'BLUE Score'을 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b6576",
   "metadata": {},
   "source": [
    "### 오류 🚨🚨🚨 ❓❓❓❓\n",
    "BLUE 점수를 구현하려는데, 스코어 점수가 0점으로 나오는 걸 보면 아예 점수를 내지 못하는데.\n",
    "\n",
    "그러한 이유를 분석해봤다.\n",
    "\n",
    "`챗봇`의 경우 질문에 대한 단 하나의 정답이 존재하지 않기 때문에 BLEU Score만으로는 완벽하게 챗봇 응답의 질을 평가할 수 없다. \n",
    "\n",
    "하지만, 챗봇 답변이 참조 응답과 얼마나 유사한 표현을 사용하는지를 평가하는 용도로 BLEU Score를 `참고`할 수 있다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4dbca89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  50%|█████     | 2/4 [00:00<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> 잠깐 쉬 어도 돼요 . <end>\n",
      "Candidate: 좋은 사람 만날 수 있을 거예요.\n",
      "BLEU Score: 0.0000\n",
      "Reference: <start> 맛난 거 드세요 . <end>\n",
      "Candidate: 잘못 주무셨나봐요.\n",
      "BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  75%|███████▌  | 3/4 [00:00<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> 떨리 겠 죠 . <end>\n",
      "Candidate: 말하기 힘들었겠어요.\n",
      "BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|██████████| 4/4 [00:00<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> 좋 아 하 면 그럴 수 있 어요 . <end>\n",
      "Candidate: 좋은 곳으로 데려다 깁 거예요.\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "Num of Samples: 4\n",
      "Average BLEU Score: 0.0000\n",
      "Average BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "# BLEU Score 계산 함수 정의\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = sentence_bleu([reference], candidate, weights=weights, smoothing_function=smoothie)\n",
    "    return bleu_score\n",
    "\n",
    "# 개별 문장에 대한 BLEU Score 계산 함수\n",
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "\n",
    "    # 모델을 사용하여 예측 답변 생성\n",
    "    predicted_answer = evaluate(src_sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    # 참조 문장과 예측 문장 토큰화\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = predicted_answer.split()\n",
    "    \n",
    "    # BLEU Score 계산\n",
    "    bleu_score = calculate_bleu(reference, candidate)\n",
    "    if verbose:\n",
    "        print(f\"Reference: {' '.join(reference)}\")\n",
    "        print(f\"Candidate: {' '.join(candidate)}\")\n",
    "        print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "# 전체 데이터셋에 대한 BLEU Score 평가 함수\n",
    "def eval_bleu(model, src_sentences, tgt_sentences, src_tokenizer, tgt_tokenizer, verbose=False):\n",
    "\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size), desc=\"Evaluating BLEU\"):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentences[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        total_score += score\n",
    "    \n",
    "    # 평균 BLEU Score 계산\n",
    "    avg_bleu_score = total_score / sample_size\n",
    "    print(f\"\\nNum of Samples: {sample_size}\")\n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    return avg_bleu_score\n",
    "\n",
    "# 예문 및 참조 답변 (테스트용)\n",
    "test_questions = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]\n",
    "\n",
    "# 예시 답변 (BLEU 계산 시 참조 답변으로 사용)\n",
    "reference_answers = [\n",
    "    \"<start> 잠깐 쉬 어도 돼요 . <end>\",\n",
    "    \"<start> 맛난 거 드세요 . <end>\",\n",
    "    \"<start> 떨리 겠 죠 . <end>\",\n",
    "    \"<start> 좋 아 하 면 그럴 수 있 어요 . <end>\"\n",
    "]\n",
    "\n",
    "# BLEU 평가 수행\n",
    "avg_bleu_score = eval_bleu(transformer, test_questions, reference_answers, tokenizer, tokenizer, verbose=True)\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37cb5b8",
   "metadata": {},
   "source": [
    "BLUE Score 구현을 잘못.. 했나?\n",
    "\n",
    "당장 시험해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "208e01b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  50%|█████     | 2/4 [00:00<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> 좋은 사람 만나 <end>\n",
      "Candidate: 좋은 사람 만날 수 있을 거예요.\n",
      "BLEU Score: 0.0972\n",
      "Reference: <start> 잘못 주무셨나요 <end>\n",
      "Candidate: 잘못 주무셨나봐요.\n",
      "BLEU Score: 0.0248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> 말하기 힘들었지만 괜찮아요 <end>\n",
      "Candidate: 말하기 힘들었겠어요.\n",
      "BLEU Score: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|██████████| 4/4 [00:00<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <start> 좋은 곳으로 데려갈거예요 <end>\n",
      "Candidate: 좋은 곳으로 데려다 깁 거예요.\n",
      "BLEU Score: 0.1212\n",
      "\n",
      "Num of Samples: 4\n",
      "Average BLEU Score: 0.0646\n",
      "Average BLEU Score: 0.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 예문 및 참조 답변 (테스트용)\n",
    "test_questions = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]\n",
    "\n",
    "# 예시 답변 (BLEU 계산 시 참조 답변으로 사용)\n",
    "reference_answers = [\n",
    "    \"<start> 좋은 사람 만나 <end>\",\n",
    "    \"<start> 잘못 주무셨나요 <end>\",\n",
    "    \"<start> 말하기 힘들었지만 괜찮아요 <end>\",\n",
    "    \"<start> 좋은 곳으로 데려갈거예요 <end>\"\n",
    "]\n",
    "\n",
    "# BLEU 평가 수행\n",
    "avg_bleu_score = eval_bleu(transformer, test_questions, reference_answers, tokenizer, tokenizer, verbose=True)\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188a789",
   "metadata": {},
   "source": [
    "예시 답변을 인위적으로 답변과 비슷하게 구현해봤는데 Score가 구해지는 것은 정상이였다 ㅎ\n",
    "\n",
    "그저 성능이 안좋았을 뿐... 그래도 챗봇인지라 챗봇답게 구현해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a95fa",
   "metadata": {},
   "source": [
    "### 민혁의 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50da8a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 민혁의 챗봇입니다. 종료하려면 '종료'라고 입력하세요.\n",
      "당신: 오늘 점심을 추천해줄래?\n",
      "챗봇: 쉽지 않은 결정이었을텐데 슬픔 갈증 많았어요.\n",
      "당신: 나 너무 배고파 뭘 먹을까?\n",
      "챗봇: 좀 더 마음이 아프네요.\n",
      "당신: 무엇을 먹을지 추천해줘\n",
      "챗봇: 끝을 아는 것도 중요한 것 같아요.\n",
      "당신: 어제 저녁을 안 먹었더니. 너무 허기져\n",
      "챗봇: 슬픈 예감은 틀린 적이 없죠.\n",
      "당신: 지금 과자 예감을 먹으라고 한거야?\n",
      "챗봇: 챙겨주고 싶나봐요.\n",
      "당신: 너 먹을 줄 아는구나?\n",
      "챗봇: 사람은 힘이 되지 않겠지만 힘내세요.\n",
      "당신: 이제 그만하자\n",
      "챗봇: 좋은 시간이 만날 수 있을 거예요.\n",
      "당신: 종료\n",
      "챗봇: 대화를 종료합니다. 좋은 하루 보내세요!\n"
     ]
    }
   ],
   "source": [
    "# 사용자 입력을 받아 챗봇 답변을 생성하는 함수\n",
    "def chatbot_response(input_text, model, src_tokenizer, tgt_tokenizer):\n",
    "    # 입력 문장 전처리\n",
    "    processed_input = preprocess_sentence(input_text)\n",
    "    \n",
    "    # 모델 예측을 위한 번역 함수 호출\n",
    "    response = evaluate(processed_input, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 챗봇 인터페이스 시작\n",
    "print(\"안녕하세요 민혁의 챗봇입니다. 종료하려면 '종료'라고 입력하세요.\")\n",
    "while True:\n",
    "    user_input = input(\"당신: \")\n",
    "    \n",
    "    if user_input.lower() == \"종료\":\n",
    "        print(\"챗봇: 대화를 종료합니다. 좋은 하루 보내세요!\")\n",
    "        break\n",
    "\n",
    "    # 모델을 통한 답변 생성\n",
    "    bot_response = chatbot_response(user_input, transformer, tokenizer, tokenizer)\n",
    "    print(f\"챗봇: {bot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7918b",
   "metadata": {},
   "source": [
    "## 회고\n",
    "이번 학습과 프로젝트에서 번역과 관련된 데이터, 모델, 성능 평가 등 많은 것들을 배워보며 실습을 진행해봤다.\n",
    "\n",
    "실습을 했던 과정을 프로젝트로 적용해보니 쉽지 않았지만.\n",
    "\n",
    "재밌었고 또한 GPT 같은 모델은 얼마나 많은 학습을 했고 많은 노력으로 만든 챗봇인지 체감이 더 되었다. 함수를 구현하는 부분이라던지 더 코드적인 면에서 모듈화하고 함수화해서 더 공부하자는 생각이 들었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c314e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e35ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e7dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed01fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9061b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310287fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea28f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
